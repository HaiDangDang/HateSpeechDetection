{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from time import time\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score,  recall_score, confusion_matrix, precision_score\n",
    "from transformers import (\n",
    "    set_seed,\n",
    ")\n",
    "set_seed(42)\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 500\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_list = [42, 57, 120, 98, 65, 74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/0_Thesis/0_final\n"
     ]
    }
   ],
   "source": [
    "if os.getcwd() == '/root':\n",
    "    new_path = \"/root/0_Thesis/0_final/\"\n",
    "    os.chdir(new_path)\n",
    "else:\n",
    "    os.chdir(\"..\") \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load Human Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84083, 7)\n"
     ]
    }
   ],
   "source": [
    "df_evaluation = pd.read_csv(\"data/human/1_combine_hate_ds.csv\")\n",
    "df_evaluation['len_text'] = df_evaluation['text'].str.len()\n",
    "df_evaluation = df_evaluation[df_evaluation['len_text'] <= 300]\n",
    "print(df_evaluation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "ViHSD           30571\n",
       "HateSpeechX     20022\n",
       "Sexism          13631\n",
       "GermEval2019    12131\n",
       "GermEval2021     3457\n",
       "Covid            2164\n",
       "US_election      2107\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hate_label_id\n",
       "1    58915\n",
       "0    25168\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation.hate_label_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "eng    37924\n",
       "vie    30571\n",
       "deu    15588\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_label\n",
       "3    1553\n",
       "1    1367\n",
       "2    1084\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation = df_evaluation[df_evaluation['dataset'] == 'HateSpeechX']\n",
    "df_evaluation['multi_label'] = 3\n",
    "#df_hate_Discrimination,offensive,hate\n",
    "df_evaluation.loc[df_evaluation['multi_label_id'] == '1', 'multi_label'] = 1\n",
    "df_evaluation.loc[df_evaluation['multi_label_id'] == '2', 'multi_label'] = 2\n",
    "df_evaluation.loc[df_evaluation['multi_label_id'] == '3', 'multi_label'] = 3\n",
    "\n",
    "\n",
    "df_evaluation = df_evaluation.sample(frac=0.2, random_state=42)\n",
    "df_evaluation['multi_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lists = [\"multi.lgb.Llama1B\", 'dangdangde/m2.multi.v3.mean.Llama1B', \"multi.lgb.Qwen14B\", \"m2.multi.v2.mean.Qwen14B\", \"\"]\n",
    "base_model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"dangdangde/\" + model_lists[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.5: Fast Qwen2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.529 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f594f2ec3a4a4d7d998f9c0e5ad9ef9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41301f3b4ab43998fce8e8deac6f47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/138M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.5 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "dtype = getattr(torch, \"bfloat16\")\n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Shot\n",
    "user_message_template = '''You are tasked with annotating speech. Your response must be a single valid number:\n",
    "1 for Hate Speech.\n",
    "2 for Offensive Speech.\n",
    "3 for Normal.\n",
    "\n",
    "Provide only the number corresponding to the category. Do not include any explanation or additional text or your thinking process.\n",
    "Do you think the following comment is hate speech or offensive speech?\n",
    "\\n\"{comment}\"\\n\n",
    "Your Answer:\n",
    "'''\n",
    "\n",
    "# Few Shot\n",
    "user_message_template = '''You are tasked with annotating speech. Your response must be a single valid number:\n",
    "    1 for Hate Speech.\n",
    "    2 for Offensive Speech.\n",
    "    3 for Normal Speech.\n",
    "\n",
    "    Think step by step:\n",
    "    - Identify whether the comment contains explicit hate towards a group or individual based on identity characteristics (Hate Speech).\n",
    "    - If the comment is aggressive, rude, or contains insults but lacks clear hateful intent, classify it as Offensive Speech.\n",
    "    - If the comment is neutral or non-offensive, classify it as Normal Speech.\n",
    "\n",
    "    Examples:\n",
    "    Comment: \"People of [group] are a disease and should be removed.\"\n",
    "    Your Answer: 1\n",
    "\n",
    "    Comment: \"You're so stupid, just shut up!\"\n",
    "    Your Answer: 2\n",
    "\n",
    "    Comment: \"I hope you have a great day!\"\n",
    "    Your Answer: 3\n",
    "\n",
    "    Provide only the number corresponding to the category. Do not include any explanation or additional text or your thinking process.\n",
    "    Do you think the following comment is hate speech or offensive speech?\n",
    "    \\n\"{comment}\"\\n\n",
    "    Your Answer:\n",
    "    '''\n",
    "def preprocess(text):\n",
    "    user_message_content = user_message_template.format(comment=text)\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message_content\n",
    "    }\n",
    "    if \"Qwen\" in model_id:\n",
    "        system_message =  {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant\"}\n",
    "    else:\n",
    "        system_message =  {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}\n",
    "\n",
    "    if \"gemma\" in model_id or \"gemma\" in model_id:\n",
    "        messages = [user_message]\n",
    "    else:\n",
    "        messages = [system_message, user_message]\n",
    "\n",
    "\n",
    "    messages = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    messages = messages\n",
    "    if \"mistral\" in model_id:\n",
    "        messages += \" \"\n",
    "\n",
    "    return messages\n",
    "\n",
    "df_evaluation[\"prompt\"] = df_evaluation['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are tasked with annotating speech. Your response must be a single valid number:\n",
      "    1 for Hate Speech.\n",
      "    2 for Offensive Speech.\n",
      "    3 for Normal Speech.\n",
      "\n",
      "    Think step by step:\n",
      "    - Identify whether the comment contains explicit hate towards a group or individual based on identity characteristics (Hate Speech).\n",
      "    - If the comment is aggressive, rude, or contains insults but lacks clear hateful intent, classify it as Offensive Speech.\n",
      "    - If the comment is neutral or non-offensive, classify it as Normal Speech.\n",
      "\n",
      "    Examples:\n",
      "    Comment: \"People of [group] are a disease and should be removed.\"\n",
      "    Your Answer: 1\n",
      "\n",
      "    Comment: \"You're so stupid, just shut up!\"\n",
      "    Your Answer: 2\n",
      "\n",
      "    Comment: \"I hope you have a great day!\"\n",
      "    Your Answer: 3\n",
      "\n",
      "    Provide only the number corresponding to the category. Do not include any explanation or additional text or your thinking process.\n",
      "    Do you think the following comment is hate speech or offensive speech?\n",
      "    \n",
      "\"the lion of jewdea starts a orginazition known as the christian resistance and we capture north america \"\n",
      "\n",
      "    Your Answer:\n",
      "    <|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_evaluation['prompt'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Qwen\" in model_id:\n",
    "    stop_token_id = tokenizer([\"123\"])['input_ids'][0]\n",
    "elif \"Llama\" in model_id or \"llama1B\" in model_id:\n",
    "    stop_token_id = [16, 17,18]\n",
    "elif \"mistral\" in model_id:\n",
    "    stop_token_id = [29508, 29518, 29538]\n",
    "    #29549\n",
    "else:\n",
    "    stop_token_id = tokenizer([\"123\"])['input_ids'][0][1:]\n",
    "assert len(stop_token_id) == 3\n",
    "    \n",
    "def process_task(texts):\n",
    "    encoding = tokenizer(texts, padding=True, return_tensors='pt').to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits  \n",
    "    last_token_logits = logits[:, -1, :] \n",
    "    probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "    indices = torch.tensor(stop_token_id)\n",
    "    probs = []\n",
    "    for i in indices:\n",
    "        probs.append( probabilities[:, i].float().cpu().numpy())\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi.lgb.Llama1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4004it [00:28, 138.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:  0.3596403596403596\n",
      "Precision:  0.305385959761406\n",
      "Recall:  0.3937718449946895\n",
      "F1:  0.2901941298580412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/root/envs/ros_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def run_test(df_for_evaluation):\n",
    "    # batch_size = 50\n",
    "    counter = 1\n",
    "    texts = []\n",
    "    device = 'cuda'\n",
    "\n",
    "    probs = []\n",
    "    for i in range(len(stop_token_id)):\n",
    "        probs.append([])\n",
    "    futures = []\n",
    "    for index, value in tqdm(enumerate(df_for_evaluation['prompt'].tolist())):\n",
    "        texts.append(value)\n",
    "\n",
    "        if len(texts) % batch_size == 0:\n",
    "        \n",
    "            prob_return = process_task(texts)   \n",
    "            for i2, p in enumerate(probs):\n",
    "                probs[i2] += prob_return[i2].tolist()\n",
    "            texts = []\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()  \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()     \n",
    "    if len(texts) != 0:\n",
    "        prob_return = process_task(texts)   \n",
    "        for i2, p in enumerate(probs):\n",
    "            probs[i2] += prob_return[i2].tolist()\n",
    "            \n",
    "        \n",
    "    y_true = df_for_evaluation['multi_label']\n",
    "    data = {}\n",
    "    for i2, p in enumerate(probs):\n",
    "        data[f\"prob_value_{i2}\"] = p\n",
    "\n",
    "    probabilities = np.array([data[\"prob_value_0\"], data[\"prob_value_1\"], data[\"prob_value_2\"]]).T  \n",
    "    y_pred = np.argmax(probabilities, axis=1)\n",
    "\n",
    "\n",
    "    print(\"ACC: \",accuracy_score(y_true - 1, y_pred))\n",
    "    print(\"Precision: \",precision_score(y_true - 1, y_pred, average=\"macro\"))\n",
    "    print(\"Recall: \",recall_score(y_true - 1, y_pred, average=\"macro\"))\n",
    "    print(\"F1: \",f1_score(y_true - 1, y_pred, average=\"macro\"))\n",
    "\n",
    "run_test(df_evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi.lgb.Qwen14B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4004it [06:55,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:  0.5591908091908092\n",
      "Precision:  0.610482753544524\n",
      "Recall:  0.5703250726332509\n",
      "F1:  0.5634568361373148\n"
     ]
    }
   ],
   "source": [
    "run_test(df_evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
