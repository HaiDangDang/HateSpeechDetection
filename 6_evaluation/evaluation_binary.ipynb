{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from time import time\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score,  recall_score, confusion_matrix, precision_score\n",
    "from transformers import (\n",
    "    set_seed,\n",
    ")\n",
    "set_seed(42)\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 500\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_list = [42, 57, 120, 98, 65, 74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/0_Thesis/0_final\n"
     ]
    }
   ],
   "source": [
    "if os.getcwd() == '/root':\n",
    "    new_path = \"/root/0_Thesis/0_final/\"\n",
    "    os.chdir(new_path)\n",
    "else:\n",
    "    os.chdir(\"..\") \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load Human Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2645, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ds\n",
       "hate     1000\n",
       "ethos     945\n",
       "hasoc     700\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RE-EVALUATION SET\n",
    "# df_evaluation = pd.read_csv(\"data/human/re_evaluation.csv\")\n",
    "# df_evaluation['len_text'] = df_evaluation['text'].str.len()\n",
    "# df_evaluation = df_evaluation[df_evaluation['len_text'] <= 300]\n",
    "# print(df_evaluation.shape)\n",
    "# df_evaluation.ds.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84083, 7)\n"
     ]
    }
   ],
   "source": [
    "df_evaluation = pd.read_csv(\"data/human/1_combine_hate_ds.csv\")\n",
    "df_evaluation['len_text'] = df_evaluation['text'].str.len()\n",
    "df_evaluation = df_evaluation[df_evaluation['len_text'] <= 300]\n",
    "print(df_evaluation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "ViHSD           30571\n",
       "HateSpeechX     20022\n",
       "Sexism          13631\n",
       "GermEval2019    12131\n",
       "GermEval2021     3457\n",
       "Covid            2164\n",
       "US_election      2107\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hate_label_id\n",
       "1    58915\n",
       "0    25168\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation.hate_label_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "eng    37924\n",
       "vie    30571\n",
       "deu    15588\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 7)\n",
      "hate_label_id\n",
      "1    3852\n",
      "0    3148\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "def create_df_for_eval(random_state=42, n = 500):\n",
    "    all_ds = np.unique(df_evaluation['dataset'])\n",
    "    df_for_evaluation = pd.DataFrame()\n",
    "    for ds in all_ds:\n",
    "        df_tmp = df_evaluation[df_evaluation['dataset'] == ds]\n",
    "        n = 500\n",
    "        if np.sum(df_tmp['hate_label_id'] == 0) <n:\n",
    "            n = np.sum(df_tmp['hate_label_id'] == 0)\n",
    "        df_tmp_0 = df_tmp[df_tmp['hate_label_id'] == 0].sample(n=n, random_state=random_state)\n",
    "        df_tmp_1 = df_tmp[df_tmp['hate_label_id'] == 1].sample(n=1000-n, random_state=random_state)\n",
    "        df_for_evaluation = pd.concat([df_for_evaluation, df_tmp_0, df_tmp_1], ignore_index=True)\n",
    "    return df_for_evaluation\n",
    "df_for_evaluation = create_df_for_eval(42)\n",
    "print(df_for_evaluation.shape)\n",
    "print(df_for_evaluation.hate_label_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       There is absolutely no way that a country with...\n",
       "1       @Johnusadollar @AnnieBurns14 What a typical, p...\n",
       "2       #coronavirus Were all going to die because som...\n",
       "3       @NortherntreeY @Lyndsayunihead @sarahgrowls @r...\n",
       "4       NOT #COVIDー19!!  IT SHOULD BE CALLED THE #Wuha...\n",
       "                              ...                        \n",
       "6495           Admin đánh rơi não trước khi đăng bài này?\n",
       "6496                                            Anh em cc\n",
       "6497    Hiến máu phải chờ xét nghiệm có kqua rồi mới đ...\n",
       "6498    No qua ma oi nguoi ta giau kg ai ma dem khoe n...\n",
       "6499    Trường Giang nói mới tỉnh ra bạn à. Chứ đâu ai...\n",
       "Name: text, Length: 3148, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_evaluation.loc[df_for_evaluation[\"hate_label_id\"] == 0,'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lists = [\"m2.v2.lgb.Llama1B\", \"m2.v2.lgb.Qwen14B\", \"dangdangde/m2.v8.lgb.Llama1B\", \"dangdangde/m2.v9.lgb.Llama1B\", 'dangdangde/m1.m2.v1.mean.Llama1B' ,'dangdangde/m2.v4.lgb.Qwen14B']\n",
    "base_model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"dangdangde/\" + model_lists[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.5: Fast Qwen2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.529 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880e2162261e4ccaa5ee515e50a5615a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.5 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "dtype = getattr(torch, \"bfloat16\")\n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_message_template = '''You are tasked with annotating speech. Your response must be a single valid number:\n",
    "1 for Hate Speech,\n",
    "2 for Normal.\n",
    "\n",
    "Provide only the number corresponding to the category. Do not include any explanation or additional text.\n",
    "Do you think the following comment is hate speech or offensive speech?\n",
    "\\n\"{comment}\"\\n\n",
    "Your Answer:\n",
    "'''\n",
    "\n",
    "def preprocess(text):\n",
    "    user_message_content = user_message_template.format(comment=text)\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message_content\n",
    "    }\n",
    "    if \"Qwen\" in model_id:\n",
    "        system_message =  {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant\"}\n",
    "    else:\n",
    "        system_message =  {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}\n",
    "\n",
    "    if \"gemma\" in model_id or \"gemma\" in model_id:\n",
    "        messages = [user_message]\n",
    "    else:\n",
    "        messages = [system_message, user_message]\n",
    "\n",
    "\n",
    "    messages = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    messages = messages\n",
    "    if \"mistral\" in model_id:\n",
    "        messages += \" \"\n",
    "\n",
    "    return messages\n",
    "\n",
    "df_evaluation[\"prompt\"] = df_evaluation['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Qwen\" in model_id:\n",
    "    stop_token_id = tokenizer([\"12\"])['input_ids'][0]\n",
    "    if \"DeepSeek\" in model_id:\n",
    "        stop_token_id = [16, 17]\n",
    "elif \"Llama\" in model_id or \"llama\" in model_id:\n",
    "    stop_token_id = [16, 17]\n",
    "    if \"DeepSeek\" in model_id:\n",
    "        stop_token_id = [16, 17]\n",
    "elif \"mistral\" in model_id or \"Mistral\" in model_id:\n",
    "    stop_token_id = [29508, 29518]\n",
    "else:\n",
    "    stop_token_id = tokenizer([\"12\"])['input_ids'][0][1:]\n",
    "\n",
    "assert len(stop_token_id) == 2\n",
    "def process_task(texts):\n",
    "    encoding = tokenizer(texts, padding=True, return_tensors='pt').to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs.logits  \n",
    "    last_token_logits = logits[:, -1, :]  \n",
    "    probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "    indices = torch.tensor(stop_token_id)\n",
    "    selected_probs_1 = probabilities[:, indices[0]].float().cpu().numpy()\n",
    "    selected_probs_2 = probabilities[:, indices[1]].float().cpu().numpy()\n",
    "    return selected_probs_1, selected_probs_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7000it [06:46, 17.23it/s]\n",
      "7000it [06:49, 17.10it/s]\n",
      "7000it [06:46, 17.21it/s]\n",
      "7000it [06:54, 16.89it/s]\n",
      "7000it [06:55, 16.85it/s]\n",
      "7000it [06:53, 16.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.79746841600499\n",
      "Acc: 0.7090714285714286\n",
      "Precision: 0.7444110532874001\n",
      "Recall: 0.5376958915713681\n",
      "f1_score: 0.6243890442962098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_y_pred = []\n",
    "all_y_true = []\n",
    "model_probs_dict = {}\n",
    "for random_state in random_state_list:\n",
    "    df_for_evaluation = create_df_for_eval(random_state)\n",
    "    df_for_evaluation[\"prompt\"] = df_for_evaluation[\"text\"].apply(preprocess)\n",
    "    probs_value_1 = []\n",
    "    probs_value_2 = []\n",
    "\n",
    "    texts = []\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    for index, value in tqdm(enumerate(df_for_evaluation['prompt'].tolist())):\n",
    "        texts.append(value)\n",
    "\n",
    "        if len(texts) % batch_size == 0:\n",
    "            selected_probs_1, selected_probs_2 = process_task(texts)   \n",
    "            probs_value_1 += selected_probs_1.tolist()\n",
    "            probs_value_2 += selected_probs_2.tolist()\n",
    "            texts = []\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    if len(texts) != 0:\n",
    "        selected_probs_1, selected_probs_2 = process_task(texts)   \n",
    "        probs_value_1 += selected_probs_1.tolist()\n",
    "        probs_value_2 += selected_probs_2.tolist()\n",
    "\n",
    "    y_true = np.array(df_for_evaluation['hate_label_id'] == 0, dtype=int)\n",
    "\n",
    "    all_y_true.extend(y_true)\n",
    "    all_y_pred.extend(probs_value_1)\n",
    "\n",
    "\n",
    "\n",
    "    model_probs_dict[random_state] = {\n",
    "        \"probs_value_1\": probs_value_1,\n",
    "        \"probs_value_2\": probs_value_2,\n",
    "        \"y_true\": y_true\n",
    "    }\n",
    "print(f\"AUC: {roc_auc_score(np.array(all_y_true), np.array(all_y_pred))}\")\n",
    "all_y_pred = np.array(all_y_pred)\n",
    "all_y_pred = all_y_pred >= 0.3\n",
    "print(f\"Acc: {accuracy_score(all_y_true , all_y_pred)}\")\n",
    "print(f\"Precision: {precision_score(all_y_true , all_y_pred)}\")\n",
    "print(f\"Recall: {recall_score(all_y_true , all_y_pred)}\")\n",
    "print(f\"f1_score: {f1_score(all_y_true , all_y_pred)}\")\n",
    "\n",
    "# prob_save_path = f\"{model_id}.pkl\"\n",
    "# with open(prob_save_path, \"wb\") as f:\n",
    "#     pickle.dump(model_probs_dict[model_id], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
