{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# For dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For LLM\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "from trl import SFTTrainer, setup_chat_format, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "import torch\n",
    "from time import time\n",
    " \n",
    "# For wandb\n",
    "import wandb\n",
    "# Set seed\n",
    "import pickle\n",
    "set_seed(42)\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers import BatchEncoding, DataCollatorForSeq2Seq\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, get_peft_model_state_dict\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "PATH = \"0_Thesis/4_Multi/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/0_Thesis/0_final\n"
     ]
    }
   ],
   "source": [
    "if os.getcwd() == '/root':\n",
    "    new_path = \"/root/0_Thesis/0_final/\"\n",
    "    os.chdir(new_path)\n",
    "else:\n",
    "    os.chdir(\"..\") \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "# model_id =\"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\"\n",
    "# model_id = \"unsloth/gemma-2-9b-it-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.529 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device supports bfloat16 but you selected float16. Will change to bfloat16.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = 500,\n",
    "    dtype = getattr(torch, \"float16\"),\n",
    "    attn_implementation='eager'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/binary/m2.v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240647, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'text', 'prompt', 'token_len',\n",
       "       'unsloth/Qwen2.5-14B-Instruct-bnb-4bit_label_1',\n",
       "       'unsloth/Qwen2.5-14B-Instruct-bnb-4bit_label_2',\n",
       "       'unsloth/gemma-2-9b-it-bnb-4bit_label_1',\n",
       "       'unsloth/gemma-2-9b-it-bnb-4bit_label_2',\n",
       "       'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit_label_1',\n",
       "       'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit_label_2',\n",
       "       'unsloth/mistral-7b-instruct-v0.3-bnb-4bit_label_1',\n",
       "       'unsloth/mistral-7b-instruct-v0.3-bnb-4bit_label_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstral7b = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
    "llama8b = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "gemma9b = \"unsloth/gemma-2-9b-it-bnb-4bit\"\n",
    "qwen14b = \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Hate Labeled:  6.03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "2    226141\n",
       "1     14506\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gemma 9B-0.5\n",
    "df[\"mean_label_1\"] = df[gemma9b+ \"_label_1\"]\n",
    "df[\"mean_label_2\"] = df[gemma9b+ \"_label_2\"]\n",
    "\n",
    "index_1 = df['mean_label_1'] > df['mean_label_2']\n",
    "index_2 = df['mean_label_1'] > .5 # Threshold selection\n",
    "index_all = index_1 & index_2\n",
    "df['label_id'] = 2\n",
    "df.loc[index_all, 'label_id'] = 1\n",
    "print(\"Percent Hate Labeled: \", round(np.sum(index_all)/df.shape[0] *100,2))\n",
    "df['label_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Hate Labeled:  1.66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "2    236660\n",
       "1      3987\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_1 = df[qwen14b+ \"_label_1\"] * .25  + df[llama8b + \"_label_1\"] * .25 + df[gemma9b+ \"_label_1\"] * .25 + df[mstral7b+ \"_label_1\"] * .25\n",
    "lb_2 = df[qwen14b+ \"_label_2\"] * .25 + df[llama8b + \"_label_2\"] * .25 + df[gemma9b+ \"_label_2\"] * .25 + df[mstral7b+ \"_label_2\"] * .25\n",
    " \n",
    "df[\"mean_label_1\"] = lb_1\n",
    "df[\"mean_label_2\"] = lb_2\n",
    "\n",
    "index_1 = df['mean_label_1'] > df['mean_label_2']\n",
    "index_2 = df['mean_label_1'] > .1 # Threshold selection\n",
    "index_all = index_1 & index_2\n",
    "df['label_id'] = 2\n",
    "df.loc[index_all, 'label_id'] = 1\n",
    "print(\"Percent Hate Labeled: \", round(np.sum(index_all)/df.shape[0] *100,2))\n",
    "df['label_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vote 2/3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Hate Labeled:  2.03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "2    235754\n",
       "1      4893\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mstral7b = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
    "llama8b = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "gemma9b = \"unsloth/gemma-2-9b-it-bnb-4bit\"\n",
    "qwen14b = \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\"\n",
    "\n",
    "index_1 = df[qwen14b+ \"_label_1\"] > df[qwen14b+ \"_label_2\"]\n",
    "index_2 = df[qwen14b+ \"_label_1\"] > .1\n",
    "index_qwen = index_1 & index_2\n",
    "\n",
    "index_1 = df[llama8b+ \"_label_1\"] > df[llama8b+ \"_label_2\"]\n",
    "index_2 = df[llama8b+ \"_label_1\"] > .1\n",
    "index_llama8b = index_1 & index_2\n",
    "\n",
    "\n",
    "index_1 = df[gemma9b+ \"_label_1\"] > df[gemma9b+ \"_label_2\"]\n",
    "index_2 = df[gemma9b+ \"_label_1\"] > .1\n",
    "index_gemma9b = index_1 & index_2\n",
    "\n",
    "index_1 = df[mstral7b+ \"_label_1\"] > df[mstral7b+ \"_label_2\"]\n",
    "index_2 = df[mstral7b+ \"_label_1\"] > .1\n",
    "index_mstral7b = index_1 & index_2\n",
    "\n",
    "index_all = index_mstral7b.astype(int) + index_gemma9b.astype(int) + index_llama8b.astype(int) + index_qwen.astype(int)\n",
    "\n",
    "index_all = index_all >= 2 #Vote2\n",
    "# index_all = index_all >= 3 #Vote3 \n",
    "\n",
    "\n",
    "df['label_id'] = 2\n",
    "\n",
    "df.loc[index_all, 'label_id'] = 1\n",
    "print(\"Percent Hate Labeled: \", round(np.sum(index_all)/df.shape[0] *100,2))\n",
    "df['label_id'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Hate Labeled:  1.15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "2    237883\n",
       "1      2764\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = ['unsloth/Qwen2.5-14B-Instruct-bnb-4bit_label_1', 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit_label_1', 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit_label_1', 'unsloth/gemma-2-9b-it-bnb-4bit_label_1', \n",
    "        ]\n",
    "with open(\"lgb/lgb_binary_label_1.pkl\", \"rb\") as f:\n",
    "    lgb_model = pickle.load(f)\n",
    "X = df[c1]\n",
    "val_preds = lgb_model.predict(X, num_iteration=lgb_model.best_iteration)\n",
    "df[\"mean_label_1\"] = val_preds\n",
    "\n",
    "with open(\"lgb/lgb_binary_label_2.pkl\", \"rb\")  as f:\n",
    "    lgb_model = pickle.load(f)\n",
    "X =df[['unsloth/Qwen2.5-14B-Instruct-bnb-4bit_label_2', 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit_label_2', 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit_label_2', 'unsloth/gemma-2-9b-it-bnb-4bit_label_2', \n",
    "       ]]\n",
    "val_preds = lgb_model.predict(X, num_iteration=lgb_model.best_iteration)\n",
    "df[\"mean_label_2\"] = val_preds\n",
    "\n",
    "index_1 = df['mean_label_1'] > df['mean_label_2']\n",
    "index_2 = df['mean_label_1'] > .1\n",
    "index_all = index_1 & index_2\n",
    "df['label_id'] = 2\n",
    "df.loc[index_all, 'label_id'] = 1\n",
    "index_lgb = deepcopy(index_all)\n",
    "\n",
    "print(\"Percent Hate Labeled: \", round(np.sum(index_all)/df.shape[0] *100,2))\n",
    "df['label_id'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_template = '''You are tasked with annotating speech. Your response must be a single valid number:\n",
    "1 for Hate Speech,\n",
    "2 for Normal Speech.\n",
    "\n",
    "Provide only the number corresponding to the category. Do not include any explanation or additional text.\n",
    "Do you think the following comment is hate speech or offensive speech?\n",
    "\\n\"{comment}\"\\n\n",
    "Your Answer:\n",
    "'''\n",
    "\n",
    "def preprocess(text, label):\n",
    "\n",
    "    user_message_content = user_message_template.format(comment=text)\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message_content\n",
    "    }\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": str(label)\n",
    "    }\n",
    "    \n",
    "    if \"Qwen\" in model_id:\n",
    "        system_message =  {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant\"}\n",
    "    else:\n",
    "        system_message =  {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}\n",
    "    if \"gemma\" in model_id or \"gemma\" in model_id:\n",
    "        messages = [user_message, assistant_message]\n",
    "        messages = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    else:\n",
    "        messages = [system_message, user_message, assistant_message]\n",
    "        messages = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    messages = messages\n",
    "    return messages\n",
    "\n",
    "df[\"prompt\"] = df.apply(lambda row: preprocess(row[\"text\"], row[\"label_id\"]), axis=1)\n",
    "comments = df[\"prompt\"].tolist()\n",
    "dataset = Dataset.from_pandas(df)\n",
    "filtered_dataset = dataset.shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = PATH + \"m2.v2.lgb.1B\"\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=output_dir, \n",
    "    eval_strategy=\"no\", \n",
    "    do_eval=False,  \n",
    "    optim=\"paged_adamw_8bit\", \n",
    "    per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=4,  \n",
    "    per_device_eval_batch_size=1,  \n",
    "    log_level=\"debug\",  \n",
    "    save_steps=1000,\n",
    "    logging_steps=200, \n",
    "    learning_rate=1e-5,  \n",
    "\n",
    "    eval_steps=5000,  \n",
    "    max_steps=50, \n",
    "    # max_steps=-1, \n",
    "\n",
    "    num_train_epochs=1, \n",
    "    warmup_steps=1, \n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    fp16=False,  \n",
    "    bf16=True, \n",
    "    max_grad_norm=0.2, \n",
    "    gradient_checkpointing=True, \n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "\n",
    "    dataset_text_field=\"prompt\",\n",
    "    max_seq_length=400, \n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.5 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\", \n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    "    use_rslora = True, \n",
    "    loftq_config = None, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ec02d919d940809f58f20ede9114a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/343709 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 8\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 343,709 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 64 | Total steps = 50\n",
      " \"-____-\"     Number of trainable parameters = 5,636,096\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwinter2109\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/0_Thesis/0_final/wandb/run-20250323_193853-04izbi3h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/winter2109/huggingface/runs/04izbi3h' target=\"_blank\">0_Thesis/4_Multi/multi.5.lgb.llama1B</a></strong> to <a href='https://wandb.ai/winter2109/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/winter2109/huggingface' target=\"_blank\">https://wandb.ai/winter2109/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/winter2109/huggingface/runs/04izbi3h' target=\"_blank\">https://wandb.ai/winter2109/huggingface/runs/04izbi3h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 02:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to 0_Thesis/4_Multi/multi.5.lgb.llama1B/checkpoint-50\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b-instruct-unsloth-bnb-4bit/snapshots/0a4436e20494a6504464ce35274b7e53fb7883d0/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\",\n",
      "      \"model.layers.1.mlp\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=2.4246192932128907, metrics={'train_runtime': 187.8131, 'train_samples_per_second': 17.038, 'train_steps_per_second': 0.266, 'total_flos': 6677722038435840.0, 'train_loss': 2.4246192932128907, 'epoch': 0.009310120100549298})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=filtered_dataset,  \n",
    "        tokenizer=tokenizer, \n",
    "        args=sft_config, \n",
    ")\n",
    "\n",
    "trainer.model.print_trainable_parameters()\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
