{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# For dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For LLM\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "from trl import SFTTrainer, setup_chat_format, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "import torch\n",
    "from time import time\n",
    " \n",
    "# For wandb\n",
    "import wandb\n",
    "# Set seed\n",
    "import pickle\n",
    "set_seed(42)\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers import BatchEncoding, DataCollatorForSeq2Seq\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, get_peft_model_state_dict\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "PATH = \"0_Thesis/4_Multi/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/0_Thesis/0_final\n"
     ]
    }
   ],
   "source": [
    "if os.getcwd() == '/root':\n",
    "    new_path = \"/root/0_Thesis/0_final/\"\n",
    "    os.chdir(new_path)\n",
    "else:\n",
    "    os.chdir(\"..\") \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "# model_id =\"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\"\n",
    "# model_id = \"unsloth/gemma-2-9b-it-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.529 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device supports bfloat16 but you selected float16. Will change to bfloat16.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = 500,\n",
    "    dtype = getattr(torch, \"float16\"),\n",
    "    attn_implementation='eager'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/multi/eng_deu_340k_v3_no_Llama8B.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "eng    218092\n",
       "deu    125617\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "3    324583\n",
       "2     18917\n",
       "1       209\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'level_0', 'index', 'text', 'prompt', 'token_len',\n",
       "       'unsloth/Qwen2.5-14B-Instruct-bnb-4bit_label_1',\n",
       "       'unsloth/Qwen2.5-14B-Instruct-bnb-4bit_label_2',\n",
       "       'unsloth/gemma-2-9b-it-bnb-4bit_label_1',\n",
       "       'unsloth/gemma-2-9b-it-bnb-4bit_label_2',\n",
       "       'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit_label_1',\n",
       "       'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit_label_2',\n",
       "       'unsloth/mistral-7b-instruct-v0.3-bnb-4bit_label_1',\n",
       "       'unsloth/mistral-7b-instruct-v0.3-bnb-4bit_label_2', 'language',\n",
       "       'Unnamed: 0', 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit_label_3',\n",
       "       'unsloth/gemma-2-9b-it-bnb-4bit_label_3',\n",
       "       'unsloth/Qwen2.5-14B-Instruct-bnb-4bit_label_3', 'label_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "3    324583\n",
       "2     18917\n",
       "1       209\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mstral7b = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
    "llama8b = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "gemma9b = \"unsloth/gemma-2-9b-it-bnb-4bit\"\n",
    "qwen14b = \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\"\n",
    "columns = [mstral7b, gemma9b, qwen14b]\n",
    "all_cols = []\n",
    "for col in columns:\n",
    "    all_cols.append(col + \"_label_1\")\n",
    "    all_cols.append(col + \"_label_2\")\n",
    "    all_cols.append(col + \"_label_3\")\n",
    "\n",
    "\n",
    "X = df[all_cols]\n",
    "\n",
    "\n",
    "with open(\"lgb/lgb_multi.pkl\", \"rb\") as f:\n",
    "    lgb_model = pickle.load(f)\n",
    "val_preds = lgb_model.predict(X, num_iteration=lgb_model.best_iteration)\n",
    "val_preds = [list(x).index(max(x)) for x in val_preds]\n",
    "df['label_id'] = val_preds\n",
    "df['label_id'] += 1\n",
    "df['label_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Model + Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "mstral7b = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
    "llama8b = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "gemma9b = \"unsloth/gemma-2-9b-it-bnb-4bit\"\n",
    "qwen14b = \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\"\n",
    "\n",
    "columns = [mstral7b, llama8b, gemma9b, qwen14b]\n",
    "columns = [mstral7b, gemma9b, qwen14b]\n",
    "\n",
    "columns_name = [\"_label_1\", \"_label_2\", \"_label_3\"]\n",
    "\n",
    "col_pred = []\n",
    "for model_id in columns:\n",
    "    col_model = []\n",
    "    for col in columns_name:\n",
    "        col_model.append(model_id + col)\n",
    "    \n",
    "    max_index = df[col_model].idxmax(axis=1).apply(lambda x: col_model.index(x))\n",
    "    df[model_id + \"_pred\"] = max_index\n",
    "    col_pred.append(model_id + \"_pred\")\n",
    "\n",
    "def majority_vote(row):\n",
    "    counts = Counter(row)\n",
    "    max_freq = max(counts.values()) \n",
    "    candidates = [key for key, val in counts.items() if val == max_freq]  \n",
    "    return min(candidates)  \n",
    "df[\"MajorityVote\"] = df[col_pred].apply(majority_vote, axis=1)\n",
    "\n",
    "col_pred = []\n",
    "for col in columns_name:\n",
    "    col_model = []\n",
    "    for model_id in columns:\n",
    "        col_model.append(model_id + col)\n",
    "    \n",
    "    mean_value = df[col_model].mean(axis=1)\n",
    "    df[col + \"_mean\"] = mean_value\n",
    "    col_pred.append(col + \"_mean\")\n",
    "max_index = df[col_pred].idxmax(axis=1).apply(lambda x: col_pred.index(x))\n",
    "df[\"mean\"] = max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean\n",
       "2    301778\n",
       "1     39668\n",
       "0      2263\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"mean\"].value_counts()\n",
    "# df['label_id'] = df[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MajorityVote\n",
       "2    298319\n",
       "1     39764\n",
       "0      5626\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"MajorityVote\"].value_counts()\n",
    "# df['label_id'] = df[\"mean\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_template = '''You are tasked with annotating speech. Your response must be a single valid number:\n",
    "    1 for Hate Speech.\n",
    "    2 for Offensive Speech.\n",
    "    3 for Normal Speech.\n",
    "\n",
    "    Think step by step:\n",
    "    - Identify whether the comment contains explicit hate towards a group or individual based on identity characteristics (Hate Speech).\n",
    "    - If the comment is aggressive, rude, or contains insults but lacks clear hateful intent, classify it as Offensive Speech.\n",
    "    - If the comment is neutral or non-offensive, classify it as Normal Speech.\n",
    "\n",
    "    Examples:\n",
    "    Comment: \"People of [group] are a disease and should be removed.\"\n",
    "    Your Answer: 1\n",
    "\n",
    "    Comment: \"You're so stupid, just shut up!\"\n",
    "    Your Answer: 2\n",
    "\n",
    "    Comment: \"I hope you have a great day!\"\n",
    "    Your Answer: 3\n",
    "\n",
    "    Provide only the number corresponding to the category. Do not include any explanation or additional text or your thinking process.\n",
    "    Do you think the following comment is hate speech or offensive speech?\n",
    "    \\n\"{comment}\"\\n\n",
    "    Your Answer:\n",
    "    '''\n",
    "\n",
    "def preprocess(text, label):\n",
    "\n",
    "    user_message_content = user_message_template.format(comment=text)\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message_content\n",
    "    }\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": str(label)\n",
    "    }\n",
    "    \n",
    "    if \"Qwen\" in model_id:\n",
    "        system_message =  {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant\"}\n",
    "    else:\n",
    "        system_message =  {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}\n",
    "    if \"gemma\" in model_id or \"gemma\" in model_id:\n",
    "        messages = [user_message, assistant_message]\n",
    "        messages = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    else:\n",
    "        messages = [system_message, user_message, assistant_message]\n",
    "        messages = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    messages = messages\n",
    "    return messages\n",
    "\n",
    "df[\"prompt\"] = df.apply(lambda row: preprocess(row[\"text\"], row[\"label_id\"]), axis=1)\n",
    "comments = df[\"prompt\"].tolist()\n",
    "dataset = Dataset.from_pandas(df)\n",
    "filtered_dataset = dataset.shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = PATH + \"multi.5.lgb.llama1B\"\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=output_dir, \n",
    "    eval_strategy=\"no\", \n",
    "    do_eval=False,  \n",
    "    optim=\"paged_adamw_8bit\", \n",
    "    per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=4,  \n",
    "    per_device_eval_batch_size=1,  \n",
    "    log_level=\"debug\",  \n",
    "    save_steps=1000,\n",
    "    logging_steps=200, \n",
    "    learning_rate=1e-5,  \n",
    "\n",
    "    eval_steps=5000,  \n",
    "    max_steps=50, \n",
    "    # max_steps=-1, \n",
    "\n",
    "    num_train_epochs=1, \n",
    "    warmup_steps=1, \n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    fp16=False,  \n",
    "    bf16=True, \n",
    "    max_grad_norm=0.2, \n",
    "    gradient_checkpointing=True, \n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "\n",
    "    dataset_text_field=\"prompt\",\n",
    "    max_seq_length=400, \n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.5 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\", \n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    "    use_rslora = True, \n",
    "    loftq_config = None, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ec02d919d940809f58f20ede9114a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/343709 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 8\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 343,709 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 64 | Total steps = 50\n",
      " \"-____-\"     Number of trainable parameters = 5,636,096\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwinter2109\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/0_Thesis/0_final/wandb/run-20250323_193853-04izbi3h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/winter2109/huggingface/runs/04izbi3h' target=\"_blank\">0_Thesis/4_Multi/multi.5.lgb.llama1B</a></strong> to <a href='https://wandb.ai/winter2109/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/winter2109/huggingface' target=\"_blank\">https://wandb.ai/winter2109/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/winter2109/huggingface/runs/04izbi3h' target=\"_blank\">https://wandb.ai/winter2109/huggingface/runs/04izbi3h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 02:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to 0_Thesis/4_Multi/multi.5.lgb.llama1B/checkpoint-50\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b-instruct-unsloth-bnb-4bit/snapshots/0a4436e20494a6504464ce35274b7e53fb7883d0/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\",\n",
      "      \"model.layers.1.mlp\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=2.4246192932128907, metrics={'train_runtime': 187.8131, 'train_samples_per_second': 17.038, 'train_steps_per_second': 0.266, 'total_flos': 6677722038435840.0, 'train_loss': 2.4246192932128907, 'epoch': 0.009310120100549298})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=filtered_dataset,  \n",
    "        tokenizer=tokenizer, \n",
    "        args=sft_config, \n",
    ")\n",
    "\n",
    "trainer.model.print_trainable_parameters()\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
